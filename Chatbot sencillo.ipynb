{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot informativo implementado al 100% en Python\n",
    "\n",
    "### Es un ejemplo sencillo de chatbot que implementa el corpus en un archivo '.txt' y que emplea las librerías nltk y scikitlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El chatbot informa a los usuarios acerca de las normas de un crucero. Es un ejemplo básico, pero que bien sirve de ejemplo de uso de lematización y búsqueda de coincidencias entre las preguntas de usuario y las diferentes respuestas posibles mediante el modelo \"cosine_similarity\"\n",
    "\n",
    "#### Resumen técnico.\n",
    "\n",
    "##### 1.- En una variable de texto se almacena el corpus (diferentes respuestas posibles al usuario).\n",
    "##### 2.- Cuando el usuario plantea una pregunta, se agrega -temporalmente- al final de la lista de respuestas. A todo este contenido se le eliminan signos de puntuación, se tokeniza, lematiza y se extraen sus caracterísaticas -mediante TfidfVectorizer de sklearn-. A partir de ellas y empleando un modelo del tipo \"cosine_similarity\" se buscan las respuestas más coincidentes con la pregunta del usuario, se elige la que mayor grado de coincidentcia muestra y se responde con ella.\n",
    "##### 3.- Adicionalmente se ha incluido un pequeño módulo de saludo inicial, que aleatoriamente elige una respuesta entre varias posibles.\n",
    "\n",
    "\n",
    "#### Próximamente subiré un sistema similar pero del tipo \"voice bot\", empleando para ello librerías de reconocimiento y síntesis de voz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download('punkt') # Instalar módulo punkt si no está ya instalado (solo ejecutar la primera vez)\n",
    "#nltk.download('wordnet') # Instalar módulo wordnet si no está ya instalado (solo ejecutar la primera vez)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1 Carga del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debe modificarse para que indique exactamente la ubicación del archivo que contiene el corpus. \n",
    "# Puede sustiruirse por un corpus sobrecualquier otra temática\n",
    "f=open(r'C:\\Corpus_crucero.txt','r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "#print(raw)  #Si deseas ver el corpus, descomenta esta línea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Definición de funciones y variables de apoyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw=raw.lower() # Convertimos todo el texto a minúsculas, para evitar deficiencias en la extracción de características\n",
    "\n",
    "sent_tokens = nltk.sent_tokenize(raw) # Convierte el corpus a una lista de sentencias\n",
    "word_tokens = nltk.word_tokenize(raw) # Convierte el corpus a una lista de palabras\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer() # Instanciamos el lematizador, con el que convertir las palabras  a sus raíces contextuales\n",
    "\n",
    "#LemTokens es una función que lematiza todos los tokens que se le pasan como parámetro\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# remove_punct es un diccionario del tipo (0signo de puntuación', None), que se emplea en la función\n",
    "# LemNormalize para sustituir los signos de puntuación por \"nada\" es decir, eliminarlos.\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "# Dado un texto como parámetro, elimina los signos de puntuación, lo convierte a minúsculas,\n",
    "# lo tokeniza -por palabras- y finalmente lo lematiza\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Preprocesamiento del texto y evaluación de la similitud entre el mensaje de usuario y las respuestas definidas en el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para determinar la similitud del texto insertado y el corpus\n",
    "def response(user_response):\n",
    "    \n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response) # Añade al final del corpus la respuesta de usuario. Se hace esto para que al usasr cosine_similarity las shapes de pregunta y respuestas coincidan y no de error\n",
    "    \n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=stopwords.words('spanish'))\n",
    "    \n",
    "    caract_textos = TfidfVec.fit_transform(sent_tokens)\n",
    "    \n",
    "    # Ahora vamos a evaluar la similitud y devolver la respuesta adecuada a partir del corpus\n",
    "    \n",
    "    # vals es un vector con los grados de coincidencia entre la pregunta y los textos.\n",
    "    # la última línea de los textos es una pregunta, por lo que en ese caso será siempre 1, nos interesa el inmediato siguiente\n",
    "    vals = cosine_similarity(caract_textos[-1], caract_textos) \n",
    "    \n",
    "    # argsort ordena en orden creciente el vector, pero devuelve los índices originales, no los valores, por lo que\n",
    "    # en idx tendremos ekl índice del término que más se ajusta como respuesta.\n",
    "    idx=vals.argsort()[0][-2] # Se ordena el vector de coincidencias (ascendente) y se toma el índice del penúltimo -el último es la pregunta de usuario-, que es el de mayor coincidencia\n",
    "    \n",
    "    flat = vals.flatten()  # eliminamos una de las dimensiones convirtiendo vals en vector\n",
    "    flat.sort()\n",
    "    nivel_coincidencia = flat[-2] # obtenemos el nivel de coincidencia para saber si hay una respuesta válida\n",
    "    \n",
    "    if(nivel_coincidencia==0):\n",
    "        robo_response=robo_response+\"Lo siento, no te he entendido. Si no puedo responder a lo que busca póngase en contacto con atención al cliente en el 902.902.902 (llamada local)\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 Definición de funcionalidades de saludo y despedida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "saludo_inputs = (\"hola\", \"buenas\", \"saludos\", \"qué tal\", \"hey\",\"buenos dias\",)\n",
    "saludo_outputs = [\"Hola\", \"Hola, ¿Qué tal?\", \"Hola, ¿Cómo te puedo ayudar?\", \"Hola, encantado de hablar contigo\"]\n",
    "\n",
    "\n",
    "despedidas = [\"Nos vemos\", \"Hasta pronto\", \"Nos vemos en otro rato\", \"Chao\", \"Bye\", \"Un placer, espero haberte ayudado\"]\n",
    "\n",
    "def saludos(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in saludo_inputs:\n",
    "            return random.choice(saludo_outputs)\n",
    "        \n",
    "def despedida():\n",
    "    return random.choice(despedidas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 Bucle conversacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBOT: Mi nombre es ROBOT. Contestaré a tus preguntas acerca de sus vacaciones en el crucero. Si quieres salir, escribe 'salir' \n",
      "¿cómo hay que vestir?\n",
      "ROBOT: para saber cómo vestir, la estación del año y la temperatura media de la zona marcan siempre la vestimenta más adecuada para los cruceros, aunque los cruceros tienden a ser cada vez más informales sobre todo durante el día.\n",
      "salir\n",
      "Bye\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"ROBOT: Mi nombre es ROBOT. Contestaré a tus preguntas acerca de sus vacaciones en el crucero. Si quieres salir, escribe 'salir' \")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower() #Convertimos a minúscula\n",
    "    \n",
    "    if(user_response!='salir'):\n",
    "        \n",
    "        if(user_response=='gracias' or user_response=='muchas gracias'): #Se podría haber definido otra función de coincidencia manual\n",
    "            flag=True\n",
    "            print(\"ROBOT: No hay de qué\")\n",
    "            \n",
    "        else:\n",
    "            if(saludos(user_response)!=None): #Si la palabra insertada por el usuario es un saludo (Coincidencias manuales definidas previamente)\n",
    "                print(\"ROBOT: \"+saludos(user_response))\n",
    "                \n",
    "            else: #Si la palabra insertada no es un saludo --> CORPUS\n",
    "                print(\"ROBOT: \",end=\"\") \n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response) # para eliminar del corpus la respuesta del usuario y volver a evaluar con el CORPUS limpio\n",
    "    else:\n",
    "        flag=False\n",
    "        print(despedida())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
